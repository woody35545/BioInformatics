{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_0613.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O4_reo7P9sP1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from os.path import join\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.python.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sequence_to_onehot(_datax):\n",
        "  # init k-mer Dictionary\n",
        "  k_mer_dict = dict()\n",
        "  k_mer = []\n",
        "  for i in product(['A', 'T', 'G', 'C', 'D'],repeat=3):\n",
        "      k_mer.append(''.join(i))\n",
        "  k_mer = pd.DataFrame(k_mer)\n",
        "  ohencoder = OneHotEncoder()\n",
        "  ohencoder.fit(k_mer)\n",
        "  k_mer_oh = ohencoder.transform(k_mer).toarray()\n",
        "  for i in range(125):\n",
        "    k_mer_dict[k_mer[0][i]] = k_mer_oh[i]\n",
        "  for i in range(125):\n",
        "    k_mer_dict[k_mer[0][i]] = list(map(int, (k_mer_dict[k_mer[0][i]])))\n",
        "  \n",
        "  # Sequence to OneHot\n",
        "  res_list = []\n",
        "  for i in range(len(_datax)):\n",
        "    tmp =list()\n",
        "    for j in range(len(_datax[i])-3):\n",
        "      tmp.append(k_mer_dict[str(_datax[i][j:j+3])])\n",
        "    res_list.append(np.array(tmp))\n",
        "  res_np = np.array(res_list)\n",
        "  return res_np\n",
        "\n",
        "def load_data(_data_file_name:str):\n",
        "  _df_shuffled = pd.read_csv(_data_file_name).sample(frac=1).reset_index(drop=True)\n",
        "  _datax = sequence_to_onehot(_df_shuffled.iloc[:,[0,]].to_numpy().reshape(-1))\n",
        "  _datay = pd.get_dummies(_df_shuffled, columns=['label'])[['label_0','label_1']][:] \n",
        "  # Train, Test Split \n",
        "  _trnx, _tstx, _trny, _tsty = train_test_split(_datax, _datay, test_size = 0.2, random_state =111)\n",
        "  return _datax, _datay, _trnx, _tstx, _trny, _tsty\n",
        "\n",
        "def make_cnn_model(input_shape:tuple):\n",
        "  _model = models.Sequential()\n",
        "  _model.add(layers.Conv2D(16,(2,2), padding='same', input_shape=input_shape))\n",
        "  _model.add(layers.BatchNormalization())\n",
        "  _model.add(layers.Activation(\"relu\"))\n",
        "  _model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "  _model.add(layers.Conv2D(16,(2,2), padding='same'))\n",
        "  _model.add(layers.BatchNormalization())\n",
        "  _model.add(layers.Activation(\"relu\"))\n",
        "  _model.add(layers.Dropout(0.2))\n",
        "  _model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "  _model.add(layers.Flatten())\n",
        "\n",
        "  _model.add(layers.Dense(units = 100, activation = \"relu\"))\n",
        "  _model.add(layers.Dense(units = 2, activation = \"sigmoid\"))\n",
        "  _model.summary()\n",
        "  return _model\n",
        "\n",
        "def fit(_model, _train_test_data:tuple, batch_size=20, epochs=50):\n",
        "  #_train_test_data = ( trnx, tstx, trny, tsty )\n",
        "  trnx, tstx, trny, tsty = _train_test_data\n",
        "  opt = optimizers.SGD(learning_rate=0.1)\n",
        "  _model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "  _history = _model.fit(trnx.reshape(-1,5597,125,1), trny, validation_data = [tstx.reshape(-1,5597,125,1),tsty], batch_size=batch_size, epochs=epochs)\n",
        "  return _history\n",
        "\n",
        "def draw_history_graph(_history):\n",
        "  plt.plot(_history.history['accuracy'])\n",
        "  plt.plot(_history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train','test'], loc = 'lower right') \n",
        "  plt.show()\n",
        "  print(f\"Final Validation Accuracy: {_history.history['val_accuracy'][-1]}\")\n",
        "\n",
        "def start_train(_data_file_name, model_to_train = None, batch_size=20, epochs=50):\n",
        "  # Load Data\n",
        "  datax, datay, trnx, tstx, trny, tsty = load_data(_data_file_name)\n",
        "  train_test_data = (trnx, tstx, trny, tsty)\n",
        "\n",
        "  # Make Model\n",
        "  if model_to_train == None:\n",
        "    model = make_cnn_model(input_shape = (5597,125,1))\n",
        "  else:\n",
        "    model = model_to_train\n",
        "\n",
        "  # Fitting Data to Model\n",
        "  history = fit(model,train_test_data,batch_size=batch_size, epochs=epochs) \n",
        "\n",
        "  # Show Performance\n",
        "  draw_history_graph(history)\n",
        "  \n",
        "  # return Trained Model & History\n",
        "  return model, history"
      ],
      "metadata": {
        "id": "ETYPJRoF90h1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#first_trained_model,first_trained_history = start_train('data800_1.csv',model_to_train = None, batch_size=50, epochs=20)"
      ],
      "metadata": {
        "id": "6U4V2eDAHQmh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import time as t \n",
        "#t.sleep(10)\n",
        "#first_trained_model.save('first_trained_model.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "0P_6mUFhMMYI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_load = models.load_model('first_trained_model.h5')"
      ],
      "metadata": {
        "id": "wtfEYSPvOa49"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datax, datay, trnx, tstx, trny, tsty = load_data('data800_2.csv')"
      ],
      "metadata": {
        "id": "w-V4MI-VQd4k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = optimizers.SGD(learning_rate=0.1)\n",
        "model_load.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "history = model_load.fit(trnx.reshape(-1,5597,125,1), trny, validation_data = [tstx.reshape(-1,5597,125,1),tsty], batch_size=20, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GND4jnaBQfD0",
        "outputId": "8d9a5b1c-d700-45dd-a811-01de54a0bee3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 15s 432ms/step - loss: 46.6625 - accuracy: 0.6812 - val_loss: 0.2373 - val_accuracy: 0.9250\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 14s 426ms/step - loss: 2.0672 - accuracy: 0.9359 - val_loss: 0.9720 - val_accuracy: 1.0000\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 0.0388 - accuracy: 0.9984 - val_loss: 0.0788 - val_accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 14s 426ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2453 - val_accuracy: 0.9250\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 14s 427ms/step - loss: 4.6219e-04 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 14s 426ms/step - loss: 4.7003e-04 - accuracy: 1.0000 - val_loss: 0.8424 - val_accuracy: 0.9688\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 3.5114e-06 - accuracy: 1.0000 - val_loss: 0.7039 - val_accuracy: 0.9750\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 14s 426ms/step - loss: 1.8358e-04 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 14s 426ms/step - loss: 3.7547e-11 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 14s 427ms/step - loss: 5.7643e-11 - accuracy: 1.0000 - val_loss: 1.7488e-04 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 14s 426ms/step - loss: 3.2458e-11 - accuracy: 1.0000 - val_loss: 1.1443e-05 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 14s 426ms/step - loss: 1.3960e-10 - accuracy: 1.0000 - val_loss: 8.5700e-07 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 14s 426ms/step - loss: 1.6518e-09 - accuracy: 1.0000 - val_loss: 9.1549e-08 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 2.7327e-11 - accuracy: 1.0000 - val_loss: 1.3147e-08 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 1.3945e-10 - accuracy: 1.0000 - val_loss: 2.7034e-09 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 1.2561e-11 - accuracy: 1.0000 - val_loss: 8.9309e-10 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 5.6293e-11 - accuracy: 1.0000 - val_loss: 4.3656e-10 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 5.1310e-09 - accuracy: 1.0000 - val_loss: 2.6884e-10 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 3.3654e-11 - accuracy: 1.0000 - val_loss: 1.9236e-10 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 2.0002e-11 - accuracy: 1.0000 - val_loss: 1.5262e-10 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#second_trained_model,second_trained_history = start_train('data800_2.csv',model_to_train=model_load, batch_size=50, epochs=20)"
      ],
      "metadata": {
        "id": "Zzi3_4gzMfYJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t.sleep(10)"
      ],
      "metadata": {
        "id": "jhp90xgXNPdh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#third_trained_model,third_trained_history = start_train('data800_3.csv',model_to_train = second_trained_model, batch_size=50, epochs=20)"
      ],
      "metadata": {
        "id": "qkKUga2iNc7c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1H6EibKTNwpN"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}